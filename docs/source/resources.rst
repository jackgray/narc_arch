=================================
External Entities and Resources
=================================

Its good to keep a tab of methods employed by other leading 
institutes. This page serves as a working reference of those 
centers and links to their resources.

Mount Sinai Icahn School of Medicine
-------------------------------------
https://icahn.mssm.edu/research/portal/resources/scientific-computing

The Scientific Computing group partners with scientists to accelerate discovery. 
With our high performance computing and data ecosystem, you can access enormous 
data sets, spend less time researching, and advance your science to discover more 
breakthroughs. 

HPC (Minerva)
    The computing infrastructure provides resources and assistance to researchers 
    across institutions around the globe. The primary asset for Scientific Computing 
    is the supercomputer Minerva, accessible for processing large-scale computing 
    jobs for research.

Data Warehouse (MSDW)
    houses and collects clinical, operational, and financial data for use in clinical 
    and translational research, containing over 10 million records and over 2 billion 
    facts.

Research Data Services 
    Researchers planning new studies or continuing longitudinal research can utilize 
    self-service tools or custom services to process, access, or collect data.

Mount Sinai Advanced Neuroimaging Research Program 
----------------------------------------------------


Berkeley Research IT 
---------------------
https://docs-research-it.berkeley.edu


XSEDE - Extreme Science & Engineering Discovery Environment
-------------------------------------------------------------
https://portal.xsede.org/allocations/resource-info

https://www.xsede.org/web/site/for-users/getting-started

XSEDE us being transitioned to NSF's ACCESS

Massachusetts Green HPC Center (MGHPCC)
----------------------------------------
a 90,000 square-foot, 15 megawatt research computing and data center facility that 
houses computing resources for five institutions: Northeastern, BU, Harvard, MIT, 
and UMass.

UMass Amherst
--------------

Northeastern University 
------------------------
https://rc.northeastern.edu

**Discovery** is a high performance computing (HPC) resource for the Northeastern 
University research community. The Discovery cluster is located in the Massachusetts 
Green High Performance Computing Center (MGHPCC) in Holyoke, MA. MGHPCC is a 90,000 
square-foot, 15 megawatt research computing and data center facility that houses 
computing resources for five institutions: Northeastern, BU, Harvard, MIT, and UMass.

Hardware 
    The Discovery cluster consists of a combination of the following CPUs and GPUs:
        2.4 GHz Intel E5-2680 v4 CPUs
        2.1 GHz Intel Xeon Platinum 8176 CPUs
        A selection of NVIDIA K80, P100, V100, and T4 GPUs
    This system provides you with access to over 20,000 CPU cores and over 200 GPUs. 
    Discovery is connected to the university network over 10 Gbps Ethernet (GbE) for 
    high-speed data transfer, and Discovery provides 3 PB of available storage on a 
    high-performance GPFS parallel filesystem. Compute nodes are connected with either 
    10 GbE or a high-performance HDR100 InfiniBand (IB) interconnect running at 100 
    Gbps, supporting all types and scales of computational workloads. Full HDR IB 
    connections (200 Gbps) are also available, if needed.



NSF Computer and Information Science & Engineering (CISE)
----------------------------------------------------------
https://www.nsf.gov/dir/index.jsp?org=CISE


Mass Open Cloud 
---------------

Reproducible Cloud 
    A core tenet of the Open Cloud eXchange model is the ability to 
    reproduce and manage federated clouds. These projects feed directly 
    into MOC operations and form the foundation for the Open Infra Labs 
    and NERC initiatives.


NERC - New England Research Cloud 
-----------------------------------
https://nerc.mghpcc.org

The vision of the New England Research Cloud (NERC) is to create a regional 
resource, rich with world-class cloud computing services. We are building a 
common cloud framework that is tailored for data-driven discovery and will 
be available to many institutions in New England. 

This sustainable partnership is aimed at bringing together research clusters 
and technology professionals.  The robust computer science research focusing 
on cloud technology coupled with strong industry collaboration,  will create 
the opportunity for the innovation space to flourish and will allow domain 
scientists to have access to technology not found elsewhere.  This approach 
sets NERC apart from today’s public clouds and is critical to bridging the 
gap in skills that are needed for research and teaching professionals. 

NERC Goals:
Build a cost effective professionally operated on-prem cloud service that 
includes various levels of services:

    -   Self-service Software-as-a-Service for easy access
    -   Automated Platform-as-a-Service for custom workflows Standardized
    -   Infrastructure-as-a-Service that includes emerging technologies for hardware acceleration
    -   Through facilitation we will increase the capabilities of:

    -   Researchers and research collaborations
    -   The emerging workforce leaving local universities
    -   The innovation hubs and start-ups connected to our universities

Set standards of deployment and automation that will allow other institutions 
to easily deploy the full suite of services built within NERC.


OpenInfra Labs (OILabs)
------------------------
https://openinfralabs.org

OpenInfra Labs (OI Labs) is the new effort established in partnership with 
Open Infrastructure Foundation and Mass Open Cloud. OILabs is created with 
the goal to expand the existing community and simplify and standardize how 
different institutions deploy and operate open source cloud infrastructure 
and cloud native software.

Initially, OILabs will prioritize the needs of the MOC and NERC environments. 
Longer term the goal is to see more organizations globally (especially in the 
academic and research space) stand up multiple consistent clouds that can enable 
hybrid and federated use cases.  If you are building or operating open source 
based clouds and would like to help standardize the process for creating them, 
we invite you to get involved and participate in OpenInfra Labs today!

A great place to see the direction OILabs is heading as we define the environment 
which will become the MOC and the NERC is at 
https://gitlab.com/open-infrastructure-labs.

We are focused on delivering open source tools to build and run cloud, container,
 AI, big data and edge workloads efficiently, repeatedly and predictably. Our 
 projects embrace three themes:

Operate First - helping developers gain insights into operational considerations 
and ensure their projects will integrate and operate well in production environments. 
Learn More.

Low-Code Development - enabling developers to build production-grade AI and big data 
applications without specialized skills and with a minimal level of coding.

Optimized Infrastructure - delivering infrastructure innovations to improve the 
performance of emerging workloads without burdening application developers.

Ohio Supercomputer Center 
--------------------------

University at(?) Buffalo Center for Computational Research 
-----------------------------------------------------------

Virginia Tech
--------------

Open On Demand
----------------
https://openondemand.org

Open OnDemand helps computational researchers and students efficiently utilize remote 
computing resources by making them easy to access from any device. It helps computer 
center staff support a wide range of clients by simplifying the user interface and 
experience.

Key Benefits & Impact
    Key benefit to you, the end user: You can use any web browser to access resources 
    at a computing service provider.

    Key benefit to you, the computer center staff: A wide range of clients/needs can 
    utilize your computing resources.

    Overall impact: Users are able to use remote computing resources faster and more 
    efficiently.

Features & More Information
    Open OnDemand is an NSF-funded open-source HPC portal based on OSC’s original 
    OnDemand portal. The goal of Open OnDemand is to provide an easy way for system 
    administrators to provide web access to their HPC resources, including, but not 
    limited to:

        -   Plugin-free web experience 
        -   Easy file management 
        -   Command-line shell access 
        -   Job management and monitoring across different batch servers and resource managers 
        -   Graphical desktop environments and desktop applications 

See the documentation for installation directions, app development tutorials, and an 
overview of the components and applications that make up OnDemand. We also have a 
walkthrough video showing the various components of an Open OnDemand instance available.

HPC Toolset Tutorial 
    https://github.com/ubccr/hpc-toolset-tutorial

    This tutorial aims to demonstrate how three open source applications work in 
    concert to provide a toolset for high performance computing (HPC) centers. 
    ColdFront is an allocations management portal that provides users an easy way 
    to request access to allocations for a Center's resources. HPC systems staff 
    configure the data center’s resources with attributes that tie ColdFront’s 
    plug-ins to systems such as job schedulers, authentication/account management 
    systems, system monitoring, and Open XDMoD. Once the user's allocation is 
    activated in ColdFront, they are able to access the resource using OnDemand, 
    a web-based portal for accessing HPC services that removes the complexities of 
    HPC system environments from the end-user. Through OnDemand, users can upload 
    and download files, create, edit, submit and monitor jobs, create and share apps, 
    run GUI applications and connect to a terminal, all via a web browser, with no 
    client software to install and configure. The Open XDMoD portal provides a rich 
    set of features, which are tailored to the role of the user. Sample metrics 
    provided by Open XDMoD include: number of jobs, CPUs consumed, wait time, and 
    wall time, with minimum, maximum and the average of these metrics. Performance 
    and quality of service metrics of the HPC infrastructure are also provided, along 
    with application specific performance metrics (flop/s, IO rates, network metrics, 
    etc) for all user applications running on a given resource.

Open XDMoD 
-----------
https://open.xdmod.org/10.0/index.html

Open XDMoD is an open source tool to facilitate the management of high performance 
computing resources. It is widely deployed at academic, industrial and governmental 
HPC centers. Open XDMoD’s management capabilities include monitoring standard metrics 
such as utilization, providing quality of service metrics designed to proactively 
identify underperforming system hardware and software, and reporting job level 
performance data for every job running on the HPC system without the need to 
recompile applications. Open XDMoD is designed to meet the following objectives: 
(1) provide the user community with a tool to more effectively and efficiently use 
their allocations and optimize their use of HPC resources, (2) provide operational 
staff with the ability to monitor, diagnose, and tune system performance as well as 
measure the performance of all applications running on their system, (3) provide 
software developers with the ability to easily obtain detailed analysis of application 
performance to aid in optimizing code performance, (4) provide stakeholders with a 
diagnostic tool to facilitate HPC planning and analysis, and (5) provide metrics to 
help measure scientific impact. In addition, analyses of the operational characteristics 
of the HPC environment can be carried out at different levels of granularity, including 
job, user, or on a system-wide basis.

The Open XDMoD portal provides a rich set of features accessible through an intuitive 
graphical interface, which is tailored to the role of the user. Metrics provided 
include: number of jobs, CPU hours consumed, wait time, and wall time, with minimum, 
maximum and the average of these metrics, in addition to many others. Metrics are 
organized by a customizable hierarchy appropriate for your organization.

A version of Open XDMoD, namely XDMoD, was developed to monitor the NSF supported 
portfolio of supercomputers that fall under the XSEDE program.

ColdFront 
----------
https://coldfront.readthedocs.io/en/latest/

ColdFront is an open source resource allocation management system designed to provide 
a central portal for administration, reporting, and measuring scientific impact of 
HPC resources. ColdFront was created to help HPC centers manage access to a diverse 
set of resources across large groups of users and provide a rich set of extensible 
meta data for comprehensive reporting. ColdFront is written in Python and released 
under the GPLv3 license.

Features
    -   Allocation based system for managing access to resources
    -   Collect Project, Grant, Publication, and other research output data from researchers
    -   Define custom attributes on resources and allocations
    -   Email notifications for expiring/renewing access to resources
    -   Integration with 3rd party systems for automation and access control
    -   Center director approval system and annual project review process


Deidentification Tools 
-----------------------
https://labs.icahn.mssm.edu/msdw/de-identified-data-service/